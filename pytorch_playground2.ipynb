{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-playground2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladwulf/ml-colab/blob/master/pytorch_playground2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AYBuki3X4dE",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1cG57BCJdxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "3bf7f08e-5846-455d-a222-3570acb9bde9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 1\n",
        "output_size = 1\n",
        "num_epochs = 60\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Toy dataset\n",
        "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
        "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
        "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
        "\n",
        "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
        "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
        "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
        "\n",
        "# Linear regression model\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    inputs = torch.from_numpy(x_train)\n",
        "    targets = torch.from_numpy(y_train)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    \n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "# Plot the graph\n",
        "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
        "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
        "plt.plot(x_train, predicted, label='Fitted line')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.ckpt')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [5/60], Loss: 10.4065\n",
            "Epoch [10/60], Loss: 4.4245\n",
            "Epoch [15/60], Loss: 2.0008\n",
            "Epoch [20/60], Loss: 1.0187\n",
            "Epoch [25/60], Loss: 0.6205\n",
            "Epoch [30/60], Loss: 0.4590\n",
            "Epoch [35/60], Loss: 0.3932\n",
            "Epoch [40/60], Loss: 0.3663\n",
            "Epoch [45/60], Loss: 0.3551\n",
            "Epoch [50/60], Loss: 0.3503\n",
            "Epoch [55/60], Loss: 0.3481\n",
            "Epoch [60/60], Loss: 0.3470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC8SvzTXx6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bz8SlipX-1Y",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekRAj8wDQmSz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "67ea46c8-2fd5-4b81-c2ad-51ab7a5f78dc"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset (images and labels)\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader (input pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "# nn.CrossEntropyLoss() computes softmax internally\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Reshape images to (batch_size, input_size)\n",
        "        images = images.reshape(-1, 28*28)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 16384/9912422 [00:00<01:27, 112667.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 29051150.41it/s]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 448343.99it/s]\n",
            "  1%|          | 16384/1648877 [00:00<00:11, 145606.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 7552379.70it/s]                            \n",
            "8192it [00:00, 180057.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Epoch [1/5], Step [100/600], Loss: 2.2375\n",
            "Epoch [1/5], Step [200/600], Loss: 2.1290\n",
            "Epoch [1/5], Step [300/600], Loss: 2.0251\n",
            "Epoch [1/5], Step [400/600], Loss: 1.9752\n",
            "Epoch [1/5], Step [500/600], Loss: 1.8990\n",
            "Epoch [1/5], Step [600/600], Loss: 1.7557\n",
            "Epoch [2/5], Step [100/600], Loss: 1.7242\n",
            "Epoch [2/5], Step [200/600], Loss: 1.7029\n",
            "Epoch [2/5], Step [300/600], Loss: 1.6339\n",
            "Epoch [2/5], Step [400/600], Loss: 1.6381\n",
            "Epoch [2/5], Step [500/600], Loss: 1.5223\n",
            "Epoch [2/5], Step [600/600], Loss: 1.4760\n",
            "Epoch [3/5], Step [100/600], Loss: 1.4397\n",
            "Epoch [3/5], Step [200/600], Loss: 1.3563\n",
            "Epoch [3/5], Step [300/600], Loss: 1.3498\n",
            "Epoch [3/5], Step [400/600], Loss: 1.3574\n",
            "Epoch [3/5], Step [500/600], Loss: 1.3548\n",
            "Epoch [3/5], Step [600/600], Loss: 1.2254\n",
            "Epoch [4/5], Step [100/600], Loss: 1.1734\n",
            "Epoch [4/5], Step [200/600], Loss: 1.2847\n",
            "Epoch [4/5], Step [300/600], Loss: 1.1559\n",
            "Epoch [4/5], Step [400/600], Loss: 1.1974\n",
            "Epoch [4/5], Step [500/600], Loss: 1.1051\n",
            "Epoch [4/5], Step [600/600], Loss: 1.0797\n",
            "Epoch [5/5], Step [100/600], Loss: 1.1061\n",
            "Epoch [5/5], Step [200/600], Loss: 0.9984\n",
            "Epoch [5/5], Step [300/600], Loss: 1.1137\n",
            "Epoch [5/5], Step [400/600], Loss: 1.0259\n",
            "Epoch [5/5], Step [500/600], Loss: 1.0351\n",
            "Epoch [5/5], Step [600/600], Loss: 1.0342\n",
            "Accuracy of the model on the 10000 test images: 82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umi3-N-eYIkV",
        "colab_type": "text"
      },
      "source": [
        "# Feed Forward NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHF60p3OYM2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "d9e591bf-2be1-4001-a49e-6b9e6a6c0318"
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3748\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3021\n",
            "Epoch [1/5], Step [300/600], Loss: 0.1969\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1232\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1196\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1455\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1495\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0727\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1070\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0693\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1567\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0799\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0101\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0901\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0628\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0337\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0523\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0314\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0469\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0467\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1038\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0310\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0334\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0285\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0278\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0352\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0299\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0087\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0084\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0679\n",
            "Accuracy of the network on the 10000 test images: 97.88 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvVjhgbtY06H",
        "colab_type": "text"
      },
      "source": [
        "# CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFZP0fPwY2uW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBFyM6TDY3YT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "433971a1-8923-4458-ed50-043046504915"
      },
      "source": [
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8773894.76it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 134919.60it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2204522.95it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 51685.26it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n",
            "Epoch [1/5], Step [100/600], Loss: 0.1344\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0832\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0689\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0615\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0724\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0560\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0199\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0140\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0363\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0783\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0456\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0088\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0681\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0805\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0555\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1001\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0929\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0344\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0138\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0423\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0084\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0958\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0108\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0413\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0528\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0161\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0461\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0096\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0182\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0179\n",
            "Test Accuracy of the model on the 10000 test images: 99.01 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-p6M5e8ascN",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifle4O8MauOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "de88e52d-425c-4114-c4b9-ac343e0ad808"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 2\n",
        "learning_rate = 0.01\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Recurrent neural network (many-to-one)\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/2], Step [100/600], Loss: 0.5057\n",
            "Epoch [1/2], Step [200/600], Loss: 0.2239\n",
            "Epoch [1/2], Step [300/600], Loss: 0.2998\n",
            "Epoch [1/2], Step [400/600], Loss: 0.0609\n",
            "Epoch [1/2], Step [500/600], Loss: 0.1309\n",
            "Epoch [1/2], Step [600/600], Loss: 0.0586\n",
            "Epoch [2/2], Step [100/600], Loss: 0.0691\n",
            "Epoch [2/2], Step [200/600], Loss: 0.1141\n",
            "Epoch [2/2], Step [300/600], Loss: 0.0941\n",
            "Epoch [2/2], Step [400/600], Loss: 0.0963\n",
            "Epoch [2/2], Step [500/600], Loss: 0.0464\n",
            "Epoch [2/2], Step [600/600], Loss: 0.1208\n",
            "Test Accuracy of the model on the 10000 test images: 97.65 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgc8Kferaupx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}